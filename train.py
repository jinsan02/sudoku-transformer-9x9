# train.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR
from torch.utils.data import DataLoader
from tqdm import tqdm

from src.config import Config
from src.data.dataset import SudokuDataset
from src.model.transformer import SudokuTransformer
from src.utils import calculate_accuracy, seed_everything

def main():
    # 1. μ΄κΈ° μ„¤μ •
    seed_everything(42)
    print(f"π”§ ν•™μµ μ¥μΉ: {Config.DEVICE} (Curriculum & Strict Mode)")
    print(f"   - λ¨λΈ μ¤ν™: d_model={Config.D_MODEL}, layers={Config.NUM_LAYERS}, head={Config.NHEAD}")
    print(f"   - λ°μ΄ν„°μ…‹: {Config.TRAIN_SIZE}κ° (ν•™μµ), {Config.VAL_SIZE}κ° (κ²€μ¦)")

    if not os.path.exists(Config.MODEL_SAVE_DIR):
        os.makedirs(Config.MODEL_SAVE_DIR)
    
    # 2. λ°μ΄ν„° λ΅λ” (Config κ²½λ΅ μ‚¬μ©)
    train_loader = DataLoader(
        SudokuDataset(f"{Config.DATA_DIR}/train.pt"), 
        batch_size=Config.BATCH_SIZE, 
        shuffle=True,
        num_workers=4,        
        pin_memory=True,      
        persistent_workers=True 
    )
    
    val_loader = DataLoader(
        SudokuDataset(f"{Config.DATA_DIR}/val.pt"), 
        batch_size=Config.BATCH_SIZE, 
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )
    
    # 3. λ¨λΈ λ° μµμ ν™” λ„κµ¬ μ„¤μ •
    model = SudokuTransformer(Config).to(Config.DEVICE)
    
    optimizer = optim.AdamW(model.parameters(), lr=Config.LR)
    criterion = nn.CrossEntropyLoss() 
    
    # [ν•µμ‹¬ μμ •] μ¤μΌ€μ¤„λ¬ μ„¤μ • (LR_MIN μ μ©)
    warmup_epochs = 3
    
    # Phase 1: μ›μ—… (0 -> 0.0005)
    scheduler1 = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs)
    
    # Phase 2: μ½”μ‚¬μΈ κ°μ† (0.0005 -> Config.LR_MIN)
    # eta_minμ„ μ„¤μ •ν•μ—¬ ν•™μµλ¥ μ΄ 0μΌλ΅ μ£½μ§€ μ•κ³  λκΉμ§€ μ μ§€λκ² ν•¨
    scheduler2 = CosineAnnealingLR(
        optimizer, 
        T_max=Config.EPOCHS - warmup_epochs, 
        eta_min=Config.LR_MIN 
    )
    
    scheduler = SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[warmup_epochs])
    
    # 4. μ²΄ν¬ν¬μΈνΈ λ΅λ“ (μ΄μ–΄ν•κΈ°)
    start_epoch = 0
    best_acc = 0.0
    
    if os.path.exists(Config.CHECKPOINT_PATH):
        print(f"π”„ μ²΄ν¬ν¬μΈνΈ λ°κ²¬! ν•™μµμ„ μ¬κ°ν•©λ‹λ‹¤: {Config.CHECKPOINT_PATH}")
        try:
            ckpt = torch.load(Config.CHECKPOINT_PATH, map_location=Config.DEVICE, weights_only=False)
            model.load_state_dict(ckpt['model_state_dict'])
            optimizer.load_state_dict(ckpt['optimizer_state_dict'])
            scheduler.load_state_dict(ckpt['scheduler_state_dict'])
            start_epoch = ckpt['epoch'] + 1
            best_acc = ckpt.get('best_acc', 0.0)
            print(f"   β–¶ Epoch {start_epoch+1}λ¶€ν„° μ‹μ‘ (ν„μ¬ μµκ³  κΈ°λ΅: {best_acc*100:.2f}%)")
        except Exception as e:
            print(f"β οΈ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨ ({e}). μ²μλ¶€ν„° λ‹¤μ‹ μ‹μ‘ν•©λ‹λ‹¤.")
    else:
        print("β¨ μƒλ΅μ΄ ν•™μµμ„ μ‹μ‘ν•©λ‹λ‹¤.")

    # 5. ν•™μµ λ£¨ν”„
    for epoch in range(start_epoch, Config.EPOCHS):
        model.train()
        train_loss = 0
        train_acc = 0
        
        # μ§„ν–‰λ¥  ν‘μ‹μ¤„ (TQDM)
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{Config.EPOCHS}")
        
        for p, s in loop:
            p, s = p.to(Config.DEVICE, non_blocking=True), s.to(Config.DEVICE, non_blocking=True)
            
            optimizer.zero_grad()
            out = model(p)
            
            # (Batch*Seq, Classes) ν•νƒλ΅ λ³€ν™ ν›„ Loss κ³„μ‚°
            loss = criterion(out.view(-1, Config.NUM_CLASSES), s.view(-1))
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_acc += calculate_accuracy(out, s)
            
            loop.set_postfix(loss=f"{loss.item():.4f}")

        # μ—ν­ λλ‚  λ•λ§λ‹¤ μ¤μΌ€μ¤„λ¬ κ°±μ‹ 
        scheduler.step()
        
        # 6. κ²€μ¦ (Validation)
        model.eval()
        val_acc = 0
        with torch.no_grad():
            for p, s in val_loader:
                p, s = p.to(Config.DEVICE), s.to(Config.DEVICE)
                val_acc += calculate_accuracy(model(p), s)
        
        avg_val_acc = val_acc / len(val_loader)
        current_lr = scheduler.get_last_lr()[0]
        
        print(f"   Done! Val Acc: {avg_val_acc*100:.2f}% | LR: {current_lr:.6f}")
        
        # 7. μ €μ¥ (μ²΄ν¬ν¬μΈνΈ & λ² μ¤νΈ λ¨λΈ)
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_acc': best_acc
        }, Config.CHECKPOINT_PATH)

        if avg_val_acc > best_acc:
            best_acc = avg_val_acc
            torch.save(model.state_dict(), Config.MODEL_PATH)
            print(f"   π† μµκ³  κΈ°λ΅ κ²½μ‹ ! λ¨λΈ μ €μ¥λ¨: {Config.MODEL_PATH}")

if __name__ == "__main__":
    main()